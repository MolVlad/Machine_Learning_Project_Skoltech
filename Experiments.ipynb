{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uw7bTjv37IED"
   },
   "outputs": [],
   "source": [
    "# Data processing tools: pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import re\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Regressors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Data imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "!pip install miceforest\n",
    "!pip install --upgrade scipy\n",
    "import miceforest as mf\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_regression = [\n",
    "    \"bias_correction_temp_forecast\",\n",
    "    \"air_quality\",\n",
    "    \"parkinson_updrs\"\n",
    "]\n",
    "\n",
    "datasets_classification = [\n",
    "    \"winequality_white\",\n",
    "    \"sensor_readings_24\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regression\")\n",
    "\n",
    "for dataset in datasets_regression:\n",
    "    df = pd.read_csv(\"Datasets/Regression/\"+dataset+\".csv\")\n",
    "    print(\"\\nName:\", dataset)\n",
    "    print(\"Shape:\",df.shape)\n",
    "    print(\"Target: from \",round(df.Target.min(),2),\" to \", round(df.Target.max(),2))\n",
    "#     print(\"Types:\",dict(df.dtypes.value_counts()))    \n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Classification\")\n",
    "\n",
    "for dataset in datasets_classification:\n",
    "    df = pd.read_csv(\"Datasets/Classification/\"+dataset+\".csv\")\n",
    "    print(\"\\nName:\", dataset)\n",
    "    print(\"Shape:\",df.shape)\n",
    "    string = \"\"\n",
    "    for i, n in enumerate(list(df.Target.value_counts())):\n",
    "        string += str(n)+\" elements, \"\n",
    "        if i == 2:\n",
    "            string = string[:-2] + \"\\n\"\n",
    "    string = string[:-2]\n",
    "    print(\"Target: \",string)\n",
    "#     print(\"Types:\",dict(df.dtypes.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(regression, dataset_number, directory = \"Datasets/\"):\n",
    "\n",
    "    if regression:\n",
    "        datasets = datasets_regression\n",
    "        directory += \"Regression/\"\n",
    "    else:\n",
    "        datasets = datasets_classification\n",
    "        directory += \"Classification/\"\n",
    "\n",
    "    dataset = datasets[dataset_number]\n",
    "\n",
    "    df = pd.read_csv(directory+dataset+\".csv\")\n",
    "    \n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_p(p):\n",
    "    return np.random.uniform() < p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(df, noise_mode, noise_level):\n",
    "    noised = df.copy()\n",
    "\n",
    "    if noise_mode == 1: # AWGN nosie\n",
    "        SNR = noise_level\n",
    "        SNR_times = 10 ** (SNR / 10.)\n",
    "\n",
    "        continuous_features =  list(df.columns[df.dtypes != \"object\"])\n",
    "        signal_powers = (abs(df[continuous_features]) ** 2).mean()\n",
    "\n",
    "        for feature in continuous_features:\n",
    "            if feature != 'Target':\n",
    "                signal_power = signal_powers[feature]\n",
    "                noise_power = signal_power / SNR_times\n",
    "                noise = np.random.normal(0,1,df.shape[0])\n",
    "                noise *= np.sqrt(noise_power)\n",
    "                noised[feature] += noise\n",
    "    elif noise_mode == 2: # replacement with some probabillity\n",
    "        changing_probability = noise_level\n",
    "\n",
    "        for i in range(df.shape[1]):\n",
    "            if df.columns[i] != 'Target':\n",
    "                for j in range(df.shape[0]):\n",
    "                    if run_with_p(changing_probability):\n",
    "                            if df.dtypes[i] == 'object':\n",
    "                                noised.iloc[j, i] = np.random.choice(df.iloc[:,i].unique())\n",
    "                            else:\n",
    "                                lim = (df.iloc[:,i].min(), df.iloc[:,i].max())\n",
    "                                noised.iloc[j, i] = np.array(np.random.uniform(*lim), dtype=df.dtypes[i])\n",
    "\n",
    "    return noised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_values(df, dropping_mode, dropping_probability):\n",
    "    with_drops = df.copy()\n",
    "    \n",
    "    if dropping_mode == 1: # MCAR\n",
    "        for i in range(df.shape[1]):\n",
    "            if df.columns[i] != 'Target':\n",
    "                for j in range(df.shape[0]):\n",
    "                    if run_with_p(dropping_probability):\n",
    "                        with_drops.iloc[j, i] = None\n",
    "    elif dropping_mode == 2: # MAR\n",
    "        Aj = with_drops.copy()\n",
    "        Aj.drop(columns=['Target'], inplace=True)\n",
    "        is_odd = np.mod(Aj.shape[1],2)\n",
    "\n",
    "        jfeat = int(np.ceil(Aj.shape[1]/2))\n",
    "        jcols = Aj.columns\n",
    "        new_jcols = np.random.choice(jcols, size = jfeat, replace = False, p = None)\n",
    "        As = Aj.drop(new_jcols, axis = 1)\n",
    "        Aj = Aj[new_jcols]\n",
    "\n",
    "        for col in range(Aj.shape[1]):\n",
    "            if is_odd:\n",
    "                if col >= As.shape[1]-1:\n",
    "                    As_index = As.shape[1]-1\n",
    "                    actual_p = 3*dropping_probability\n",
    "                else:\n",
    "                    As_index = col\n",
    "                    actual_p = 4*dropping_probability\n",
    "            else:\n",
    "                As_index = col\n",
    "                actual_p = 4*dropping_probability\n",
    "\n",
    "            if As.dtypes[As_index] != 'object':\n",
    "                med = As.iloc[:,As_index].median()\n",
    "\n",
    "                if np.random.randint(2):\n",
    "                    for row in range(Aj.shape[0]):\n",
    "                        if As.iloc[row, As_index] < med:\n",
    "                            if run_with_p(actual_p):\n",
    "                                with_drops.loc[row, new_jcols[col]]=None\n",
    "                else:\n",
    "                    for row in range(Aj.shape[0]):\n",
    "                        if As.iloc[row, As_index] > med:\n",
    "                            if run_with_p(actual_p):\n",
    "                                with_drops.loc[row, new_jcols[col]]=None\n",
    "            else:\n",
    "                classes=As.iloc[:,As_index].unique()\n",
    "                subset_size = int(np.floor(len(classes)/2))\n",
    "                subset=np.random.choice(classes, size = subset_size, replace = False, p = None)\n",
    "\n",
    "                for row in range(Aj.shape[0]):\n",
    "                    if As.iloc[row, As_index] in subset:\n",
    "                        if run_with_p(actual_p):\n",
    "                            with_drops.loc[row, new_jcols[col]]=None\n",
    "    elif dropping_mode == 3: # NMAR\n",
    "        Aj = with_drops.copy()\n",
    "        Aj.drop(columns=['Target'], inplace=True)\n",
    "        jcols = Aj.columns\n",
    "        \n",
    "        for col in range(Aj.shape[1]):\n",
    "            if Aj.dtypes[col] != 'object':\n",
    "                med = Aj.iloc[:,col].median()\n",
    "                if np.random.randint(2):\n",
    "                    for row in range(Aj.shape[0]):\n",
    "                        if Aj.iloc[row, col] < med:\n",
    "                            if run_with_p(2*dropping_probability):\n",
    "                                with_drops.loc[row, jcols[col]]=None\n",
    "                else:\n",
    "                    for row in range(Aj.shape[0]):\n",
    "                        if Aj.iloc[row, col] > med:\n",
    "                            if run_with_p(2*dropping_probability):\n",
    "                                with_drops.loc[row, jcols[col]]=None\n",
    "            else:\n",
    "                classes=Aj.iloc[:,col].unique()\n",
    "                subset_size = int(np.floor(len(classes)/2))\n",
    "                subset=np.random.choice(classes, size = subset_size, replace = False, p = None)\n",
    "                for row in range(Aj.shape[0]):\n",
    "                    if Aj.iloc[row, col] in subset:\n",
    "                        if run_with_p(2*dropping_probability):\n",
    "                            with_drops.loc[row, jcols[col]]=None\n",
    "                        \n",
    "    return with_drops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(df, imputation_mode):    \n",
    "    if imputation_mode == 0: # dropping\n",
    "        filled = df.dropna()\n",
    "    elif imputation_mode == 1: # filling with 0\n",
    "        filled = df.fillna(0)\n",
    "    elif imputation_mode == 2: # filling with mean\n",
    "        filled = df.copy()\n",
    "\n",
    "        categorical_features = list(df.columns[df.dtypes == \"object\"])\n",
    "        continuous_features =  list(df.columns[df.dtypes != \"object\"])\n",
    "\n",
    "        for feature in categorical_features:\n",
    "            filled[feature].fillna(filled[feature].value_counts().index[0], inplace=True)\n",
    "        for feature in continuous_features:\n",
    "            filled[feature].fillna(filled[feature].mean(), inplace=True)\n",
    "    elif imputation_mode == 3: # filling with median\n",
    "        filled = df.copy()\n",
    "\n",
    "        categorical_features = list(df.columns[df.dtypes == \"object\"])\n",
    "        continuous_features =  list(df.columns[df.dtypes != \"object\"])\n",
    "\n",
    "        for feature in categorical_features:\n",
    "            filled[feature].fillna(filled[feature].value_counts().index[0], inplace=True)\n",
    "        for feature in continuous_features:\n",
    "            filled[feature].fillna(filled[feature].median(), inplace=True)\n",
    "    elif imputation_mode == 4: # filling by MICE\n",
    "        y = df['Target']\n",
    "        X = df.drop('Target', axis = 1)\n",
    "\n",
    "        kds = mf.ImputationKernel(\n",
    "          X,\n",
    "          datasets=1,\n",
    "          save_all_iterations=False,\n",
    "          random_state=random_state\n",
    "        )\n",
    "\n",
    "        kds.mice(5)\n",
    "\n",
    "        X_filled = kds.complete_data(dataset=0, inplace=False)\n",
    "\n",
    "        filled = pd.concat([X_filled,y], axis=1)\n",
    "    elif imputation_mode == 5: # filling by KNN\n",
    "        y = df['Target']\n",
    "        X = df.drop('Target', axis = 1)\n",
    "\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        X_filled = imputer.fit_transform(X)\n",
    "\n",
    "        for col in range(X.shape[1]):\n",
    "            X.iloc[:,col] = X_filled[:,col]\n",
    "\n",
    "        filled = pd.concat([X,y], axis=1)\n",
    "        \n",
    "    return filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kLMv2qzuQlX"
   },
   "outputs": [],
   "source": [
    "def get_models_and_params(regression, random_state):\n",
    "    # Models used\n",
    "    if regression:\n",
    "        models = [\n",
    "            LinearRegression(n_jobs=-1),\n",
    "            DecisionTreeRegressor(random_state = random_state),\n",
    "            RandomForestRegressor(random_state = random_state, n_jobs = -1),\n",
    "            LGBMRegressor(random_state = random_state, n_jobs = -1),\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            LogisticRegression(n_jobs=-1),\n",
    "            DecisionTreeClassifier(random_state = random_state),\n",
    "            RandomForestClassifier(random_state = random_state, n_jobs = -1),\n",
    "            LGBMClassifier(random_state = random_state, n_jobs = -1),\n",
    "        ]\n",
    "\n",
    "    # Parameters grid\n",
    "    grid_search_parameters = {\n",
    "        'n_estimators': list(range(10,100,10)),\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'C': np.logspace(-3,3,7),\n",
    "        'kernel': ['poly', 'rbf'], \n",
    "        }\n",
    "    \n",
    "    return models, grid_search_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(df, regression):\n",
    "    if regression:\n",
    "        y = df['Target']\n",
    "    else:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        y = encoder.fit_transform(df['Target'])\n",
    "\n",
    "    X = pd.get_dummies(df.drop('Target', axis = 1), drop_first = False)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getkeys(dict): \n",
    "    return [*dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparams(X, y, models, grid_search_parameters, scoring, verbose=False):\n",
    "    best_models = []\n",
    "    val_scores = []\n",
    "        \n",
    "    for model in models:\n",
    "        # Intersection of keys for GridSearchCV\n",
    "        intersection_keys = model.get_params().keys() & grid_search_parameters.keys()\n",
    "        parameters_grid = {}\n",
    "        for key in intersection_keys:\n",
    "            parameters_grid[key] = grid_search_parameters[key]\n",
    "\n",
    "        # Create pipeline\n",
    "        pln = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "        # Update parameter names\n",
    "        pipeline_params_grid = {}\n",
    "        for i in range(len(parameters_grid)):\n",
    "            pipeline_params_grid[str(pln.steps[1][0]) + '__' + str(getkeys(parameters_grid)[i])] = \\\n",
    "                parameters_grid[str(getkeys(parameters_grid)[i])]    \n",
    "\n",
    "        # Cross-validation\n",
    "        gcv = GridSearchCV(pln, pipeline_params_grid, scoring = scoring, cv = 3,\n",
    "                           n_jobs = -1, refit = True)\n",
    "        gcv.fit(X, y)\n",
    "\n",
    "        if verbose:\n",
    "            print(model.__class__.__name__, ' best with ', gcv.best_params_)\n",
    "        \n",
    "        best_models.append(gcv.best_estimator_)\n",
    "        val_scores.append(abs(cross_val_score(gcv.best_estimator_, X, y, scoring = scoring, cv=3).mean()))\n",
    "\n",
    "    return best_models, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kLMv2qzuQlX"
   },
   "outputs": [],
   "source": [
    "def eval_dataset(X, y, models, scoring):    \n",
    "    val_scores = []\n",
    "    \n",
    "    for model in models:\n",
    "        val_scores.append(abs(cross_val_score(model, X, y, scoring = scoring, cv=3).mean()))\n",
    "\n",
    "    return val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters description\n",
    "#### Noise mode\n",
    "- 0 - no noise\n",
    "- 1 - AWGN noise\n",
    "- 2 - replacements\n",
    "\n",
    "#### Dropping mode\n",
    "- 0 - no dropping\n",
    "- 1 - MCAR\n",
    "- 2 - MAR\n",
    "- 3 - NMAR\n",
    "\n",
    "#### Imputation mode\n",
    "- 0 - dropping\n",
    "- 1 - filling with 0\n",
    "- 2 - filling with mean\n",
    "- 3 - filling with median\n",
    "- 4 - filling by MICE\n",
    "- 5 - filling by kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_with_results = \"Results\"\n",
    "\n",
    "# Create directory\n",
    "if not os.path.exists(directory_with_results):\n",
    "    os.mkdir(directory_with_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with noise only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For resulting csv\n",
    "random_states = []\n",
    "dataset_numbers = []\n",
    "regressions = []\n",
    "\n",
    "noise_modes = []\n",
    "noise_levels = []\n",
    "\n",
    "val_scores = [[],[],[],[]]\n",
    "initial_val_scores = [[],[],[],[]]\n",
    "\n",
    "for random_state in range(3):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    start = time.time()\n",
    "    print(\"random state\", random_state)\n",
    "\n",
    "    for regression in [True, False]:\n",
    "        if regression:\n",
    "            scoring = 'neg_mean_absolute_percentage_error'\n",
    "        else:\n",
    "            scoring = 'f1_micro'\n",
    "            \n",
    "        models, grid_search_parameters = get_models_and_params(regression, random_state)\n",
    "                                \n",
    "        for dataset_number in [0,1,2]:\n",
    "            \n",
    "            if (dataset_number == 2) and (regression == False):\n",
    "                continue\n",
    "            \n",
    "            df = read_dataset(regression, dataset_number)\n",
    "            print(int(time.time() - start), \"seconds\", \"read dataset\", dataset_number, \\\n",
    "                  \"regression\", regression)\n",
    "\n",
    "            X, y = get_X_y(df, regression)\n",
    "\n",
    "            best_models, initial_val_scores_per_run = tune_hyperparams(X, y, models, grid_search_parameters, \\\n",
    "                                                             scoring)\n",
    "            \n",
    "            for array_of_noise_levels, noise_mode in [(np.linspace(0, 0.5, 21), 2), (range(-20, 21, 2), 1)]:\n",
    "                for noise_level in array_of_noise_levels:\n",
    "                    \n",
    "                    noised = add_noise(df, noise_mode, noise_level)\n",
    "\n",
    "                    X, y = get_X_y(noised, regression)\n",
    "\n",
    "                    val_scores_per_run = eval_dataset(X, y, best_models, scoring)\n",
    "                    \n",
    "                    random_states.append(random_state)\n",
    "                    dataset_numbers.append(dataset_number)\n",
    "                    regressions.append(regression)\n",
    "\n",
    "                    noise_modes.append(noise_mode)\n",
    "                    noise_levels.append(noise_level)\n",
    "\n",
    "                    for i in range(len(models)):\n",
    "                        initial_val_scores[i].append(initial_val_scores_per_run[i])\n",
    "                        val_scores[i].append(val_scores_per_run[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "           'Random State': random_states,\n",
    "           'Dataset number': dataset_numbers,\n",
    "           'Is regression': regressions,\n",
    "    \n",
    "           'Noise mode': noise_modes,\n",
    "           'Noise level': noise_levels,\n",
    "\n",
    "           'Val score 0': val_scores[0],\n",
    "           'Val score 1': val_scores[1],\n",
    "           'Val score 2': val_scores[2],\n",
    "           'Val score 3': val_scores[3],\n",
    "    \n",
    "           'Initial val score 0': initial_val_scores[0],\n",
    "           'Initial val score 1': initial_val_scores[1],\n",
    "           'Initial val score 2': initial_val_scores[2],\n",
    "           'Initial val score 3': initial_val_scores[3],\n",
    "           })\n",
    "\n",
    "results.to_csv(directory_with_results+\"/results_noise_only.csv\", index=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbS9_ub7G8zd"
   },
   "source": [
    "# Experiments with missing values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For resulting csv\n",
    "random_states = []\n",
    "dataset_numbers = []\n",
    "regressions = []\n",
    "\n",
    "dropping_modes = []\n",
    "dropping_level = []\n",
    "imputation_modes = []\n",
    "\n",
    "val_scores = [[],[],[],[]]\n",
    "initial_val_scores = [[],[],[],[]]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for random_state in range(3):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    print(\"random state\", random_state)\n",
    "\n",
    "    for regression in [True, False]:\n",
    "        if regression:\n",
    "            scoring = 'neg_mean_absolute_percentage_error'\n",
    "        else:\n",
    "            scoring = 'f1_micro'\n",
    "            \n",
    "        models, grid_search_parameters = get_models_and_params(regression, random_state)\n",
    "                       \n",
    "        for dataset_number in [0,1,2]:\n",
    "            \n",
    "            if (dataset_number == 2) and (regression == False):\n",
    "                continue\n",
    "            \n",
    "            df = read_dataset(regression, dataset_number)\n",
    "            print(int(time.time() - start), \"seconds\", \"read dataset\", dataset_number, \"regression\", regression)\n",
    "\n",
    "            X, y = get_X_y(df, regression)\n",
    "\n",
    "            best_models, initial_val_scores_per_run = tune_hyperparams(X, y, models, grid_search_parameters, \\\n",
    "                                                             scoring)\n",
    "            \n",
    "            for dropping_mode in [1,2,3]:\n",
    "                for dropping_probability in [0.1, 0.15, 0.2, 0.25, 0.3]:\n",
    "\n",
    "                    with_drops = drop_values(df, dropping_mode, dropping_probability)\n",
    "                \n",
    "                    for imputation_mode in [1,2,3,4,5]:\n",
    "                        imputed = impute_data(with_drops, imputation_mode)\n",
    "\n",
    "                        X, y = get_X_y(imputed, regression)\n",
    "\n",
    "                        val_scores_per_run = eval_dataset(X, y, best_models, scoring)\n",
    "                    \n",
    "                        random_states.append(random_state)\n",
    "                        dataset_numbers.append(dataset_number)\n",
    "                        regressions.append(regression)\n",
    "\n",
    "                        dropping_modes.append(dropping_mode)\n",
    "                        dropping_level.append(dropping_probability)\n",
    "                        imputation_modes.append(imputation_mode)\n",
    "\n",
    "                        for i in range(len(models)):\n",
    "                            initial_val_scores[i].append(initial_val_scores_per_run[i])\n",
    "                            val_scores[i].append(val_scores_per_run[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "           'Random State': random_states,\n",
    "           'Dataset number': dataset_numbers,\n",
    "           'Is regression': regressions,\n",
    "    \n",
    "           'Drop mode': dropping_modes,\n",
    "           'Drop level': dropping_level,\n",
    "           'Imputation mode' : imputation_modes,\n",
    "\n",
    "           'Val score 0': val_scores[0],\n",
    "           'Val score 1': val_scores[1],\n",
    "           'Val score 2': val_scores[2],\n",
    "           'Val score 3': val_scores[3],\n",
    "    \n",
    "           'Initial val score 0': initial_val_scores[0],\n",
    "           'Initial val score 1': initial_val_scores[1],\n",
    "           'Initial val score 2': initial_val_scores[2],\n",
    "           'Initial val score 3': initial_val_scores[3],\n",
    "           })\n",
    "\n",
    "results.to_csv(directory_with_results+\"/results_drop_only.csv\", index=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbS9_ub7G8zd"
   },
   "source": [
    "# Experiments with missing values and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For resulting csv\n",
    "random_states = []\n",
    "dataset_numbers = []\n",
    "regressions = []\n",
    "\n",
    "noise_modes = []\n",
    "noise_levels = []\n",
    "\n",
    "dropping_modes = []\n",
    "dropping_level = []\n",
    "imputation_modes = []\n",
    "\n",
    "val_scores = [[],[],[],[]]\n",
    "initial_val_scores = [[],[],[],[]]\n",
    "\n",
    "for random_state in range(3):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    start = time.time()\n",
    "    print(\"random state\", random_state)\n",
    "\n",
    "    for regression in [True, False]:\n",
    "        if regression:\n",
    "            scoring = 'neg_mean_absolute_percentage_error'\n",
    "        else:\n",
    "            scoring = 'f1_micro'\n",
    "            \n",
    "        models, grid_search_parameters = get_models_and_params(regression, random_state)\n",
    "                                \n",
    "        for dataset_number in [0,1,2]:\n",
    "            \n",
    "            if (dataset_number == 2) and (regression == False):\n",
    "                continue\n",
    "            \n",
    "            df = read_dataset(regression, dataset_number)\n",
    "            print(int(time.time() - start), \"seconds\", \"read dataset\", dataset_number, \\\n",
    "                  \"regression\", regression)\n",
    "\n",
    "            X, y = get_X_y(df, regression)\n",
    "\n",
    "            best_models, initial_val_scores_per_run = tune_hyperparams(X, y, models, grid_search_parameters, \\\n",
    "                                                             scoring)\n",
    "            \n",
    "            for array_of_noise_levels, noise_mode in [([0.1, 0.2, 0.3],2), ([-6, 0, 10], 1)]:\n",
    "                for noise_level in array_of_noise_levels:\n",
    "                    noised = add_noise(df, noise_mode, noise_level)\n",
    "                \n",
    "                    for dropping_mode in [1,2,3]:\n",
    "                        for dropping_probability in [0.1, 0.2, 0.3]:\n",
    "\n",
    "                            with_drops = drop_values(noised, dropping_mode, dropping_probability)\n",
    "\n",
    "                            for imputation_mode in [1,2,3,4,5]:\n",
    "                                imputed = impute_data(with_drops, imputation_mode)\n",
    "\n",
    "                                X, y = get_X_y(imputed, regression)\n",
    "\n",
    "                                val_scores_per_run = eval_dataset(X, y, best_models, scoring)\n",
    "                    \n",
    "                                random_states.append(random_state)\n",
    "                                dataset_numbers.append(dataset_number)\n",
    "                                regressions.append(regression)\n",
    "                                \n",
    "                                noise_modes.append(noise_mode)\n",
    "                                noise_levels.append(noise_level)\n",
    "\n",
    "                                dropping_modes.append(dropping_mode)\n",
    "                                dropping_level.append(dropping_probability)\n",
    "                                imputation_modes.append(imputation_mode)\n",
    "\n",
    "                                for i in range(len(models)):\n",
    "                                    initial_val_scores[i].append(initial_val_scores_per_run[i])\n",
    "                                    val_scores[i].append(val_scores_per_run[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "           'Random State': random_states,\n",
    "           'Dataset number': dataset_numbers,\n",
    "           'Is regression': regressions,\n",
    "    \n",
    "           'Noise mode': noise_modes,\n",
    "           'Noise level': noise_levels,\n",
    "    \n",
    "           'Drop mode': dropping_modes,\n",
    "           'Drop level': dropping_level,\n",
    "           'Imputation mode' : imputation_modes,\n",
    "\n",
    "           'Val score 0': val_scores[0],\n",
    "           'Val score 1': val_scores[1],\n",
    "           'Val score 2': val_scores[2],\n",
    "           'Val score 3': val_scores[3],\n",
    "    \n",
    "           'Initial val score 0': initial_val_scores[0],\n",
    "           'Initial val score 1': initial_val_scores[1],\n",
    "           'Initial val score 2': initial_val_scores[2],\n",
    "           'Initial val score 3': initial_val_scores[3],\n",
    "           })\n",
    "\n",
    "results.to_csv(directory_with_results+\"/results_noise_and_drop.csv\", index=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project_part_Stepan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
